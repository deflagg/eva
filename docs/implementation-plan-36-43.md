## docs/implementation-plan-36-43.md — Eva Speech Endpoint + Auto Speak via `tts_response`

Implement the system below in SMALL ITERATIONS so diffs stay small and reviewable. Do NOT do big refactors. Do NOT “get ahead” of the current iteration. Each iteration must end with:

* build/lint/typecheck passing (or explicit “no tests yet; manual test steps included”)
* a short change summary + files changed
* clear run instructions
* stop after each iteration to allow review before proceeding
* keep progress in progress.md

---

# GOAL

1. Eva exposes speech synthesis via HTTP:

* `POST /speech` → returns MP3 audio bytes

2. VisionAgent adds a new field:

* `insight.summary.tts_response` (1–2 sentence “normal human response”)

3. UI:

* shows `tts_response` in the Latest Insight panel
* **auto-speaks** the `tts_response` for new insights (with autoplay-safe gating)

---

# DECISIONS (LOCKED)

* Speech transport v1: **HTTP POST** (binary response). No WS audio streaming in v1.
* Spoken text source: **`insight.summary.tts_response`** only (UI does not generate its own narration).
* `tts_response` is generated by **VisionAgent** and propagated through QuickVision → UI types.
* Browser autoplay reality: UI must include a one-time **“Enable Audio”** interaction to unlock playback.

---

# API CONTRACT (Speech v1)

### `POST /speech`

Request JSON:

```json
{ "text": "…", "voice": "en-US-JennyNeural", "rate": 1.0 }
```

Response:

* `200` `Content-Type: audio/mpeg` body bytes

Errors:

* `400` invalid JSON / invalid fields
* `413` body too large
* `429` cooldown active
* `500` synthesis failed

CORS (dev-friendly):

* Support `OPTIONS /speech`
* For `/speech` responses:

  * `Access-Control-Allow-Origin: *`
  * `Access-Control-Allow-Methods: POST, OPTIONS`
  * `Access-Control-Allow-Headers: content-type`

---

# INSIGHT CONTRACT CHANGE

Add:

* `InsightMessage.summary.tts_response: string`

`tts_response` rules (LOCKED):

* 1–2 sentences
* spoken-friendly, natural
* no tags, no IDs, no token/cost, no telemetry
* severity-aware tone (calm for LOW, urgent for HIGH)

---

# EVA CONFIG (new)

Extend `packages/eva/src/config.ts` + `packages/eva/eva.config.json` with:

```json
"speech": {
  "enabled": false,
  "path": "/speech",
  "defaultVoice": "en-US-JennyNeural",
  "maxTextChars": 1000,
  "maxBodyBytes": 65536,
  "cooldownMs": 0,
  "cache": { "enabled": true, "ttlMs": 600000, "maxEntries": 64 }
}
```

Also add `packages/eva/eva.config.local.example.json` (copy-only) enabling speech.

---

# UI AUTO-SPEAK POLICY (v1)

Speak when a *new* insight arrives and:

* autoSpeak enabled
* `severity >= MED` by default (configurable)
* cooldown not violated (e.g., 2 seconds)
* `tts_response` non-empty

Speak text:

* exactly `insight.summary.tts_response`

Never speak:

* tags, IDs, usage, cost

---

# IMPLEMENTATION ITERATIONS (START AT 36)

## Iteration 36 — Eva config plumbing only (speech config; no runtime behavior)

Goal:

* Add `speech` block to Eva config schema + committed defaults without changing runtime unless enabled.

Deliverables:

* Update `packages/eva/src/config.ts` to add `speech` schema (Zod), default `enabled:false`
* Update `packages/eva/eva.config.json` to include `speech` block (enabled=false)
* Add `packages/eva/eva.config.local.example.json` enabling speech (copy-only)

Acceptance:

* `cd packages/eva && npm run build` passes
* Running Eva with only `eva.config.json` behaves exactly as before

Stop; update progress.md.

---

## Iteration 37 — Add Edge TTS dependency + wrapper module (no endpoint yet)

Goal:

* Add synthesis engine behind a small stable API.

Deliverables:

* Add `node-edge-tts` to `packages/eva/package.json` (pin version)
* Add:

  * `packages/eva/src/speech/edgeTts.ts` exporting `synthesize({text, voice, rate}) => Promise<Buffer>`
  * `packages/eva/src/speech/types.ts`
* Handle ESM/CJS interop (dynamic import if needed)

Acceptance:

* `cd packages/eva && npm i && npm run build` passes

Stop; update progress.md.

---

## Iteration 38 — Eva HTTP router + `POST /speech` returns MP3 bytes (MVP)

Goal:

* Expose `/speech` when enabled, with guardrails + CORS.

Deliverables:

* Update `packages/eva/src/server.ts` to add a tiny router:

  * `OPTIONS <speech.path>` → 204 + CORS headers
  * `POST <speech.path>` → read body (maxBodyBytes) → parse JSON → validate → synthesize → return `audio/mpeg`
  * fallback: preserve existing default JSON response for all other routes (do not break current behavior)
* Extend `StartServerOptions` to accept `speech` config
* Update `packages/eva/src/index.ts` to pass `speech` config into `startServer(...)`

Guardrails (required):

* enforce `maxBodyBytes` while reading request stream → `413`
* validate `text` non-empty and `<= maxTextChars` → `400`
* optional server cooldown (`cooldownMs`) → `429`

Manual acceptance:

```bash
# enable speech in eva.config.local.json (copy from example)
cd packages/eva && npm run dev

curl -sS -X POST http://127.0.0.1:8787/speech \
  -H 'content-type: application/json' \
  -d '{"text":"hello from eva","voice":"en-US-JennyNeural"}' \
  --output out.mp3
```

`out.mp3` plays.

Stop; update progress.md.

---

## Iteration 39 — VisionAgent: add `tts_response` to tool schema + prompt guidance

Goal:

* VisionAgent returns `summary.tts_response`.

Deliverables:

* Update `packages/vision-agent/src/tools.ts`:

  * extend InsightSummary schema to include `tts_response: string` (minLength 1)
  * update tool description to include `tts_response`
* Update `packages/vision-agent/src/prompts.ts`:

  * add strict instruction for `tts_response` (1–2 sentences, spoken-friendly, no telemetry)
* Update `packages/vision-agent/README.md` to document the new response field

Acceptance:

* `cd packages/vision-agent && npm run build`
* Manual: calling VisionAgent insight endpoint returns `tts_response` in the summary

Stop; update progress.md.

---

## Iteration 40 — QuickVision: propagate `tts_response` through schemas + WS message

Goal:

* QuickVision includes `tts_response` in its insight message sent to UI.

Deliverables:

* Update `packages/quickvision/app/vision_agent_client.py` to parse/validate `tts_response`
* Update `packages/quickvision/app/protocol.py` to include `tts_response` in `InsightSummary`
* Ensure insight relay emits it unchanged

Acceptance:

* QuickVision still runs
* Trigger insight → message includes `tts_response`

Stop; update progress.md.

---

## Iteration 41 — UI: add `tts_response` to types + display it in Latest Insight panel

Goal:

* UI recognizes and shows `tts_response`.

Deliverables:

* Update `packages/ui/src/types.ts` to include `tts_response` on `InsightSummary`
* Update `packages/ui/src/main.tsx` Latest Insight section:

  * display `latestInsight.summary.tts_response` as the “spoken line” (under one_liner or replacing it—keep both visible for now)

Acceptance:

* `cd packages/ui && npm run build`
* Manual: latest insight UI shows tts_response when present

Stop; update progress.md.

---

## Iteration 42 — UI: speech client + “Enable Audio” (autoplay unlock)

Goal:

* UI can request MP3 from Eva and play it reliably.

Deliverables:

* Derive Eva HTTP base from `eva.wsUrl`:

  * `ws://host:port/eye` → `http://host:port`
  * `wss://…` → `https://…`
  * strip any path
* Add `packages/ui/src/speech.ts`:

  * `speakText(text, voice?)`:

    * POST `/speech`
    * `blob()` → `URL.createObjectURL` → set `<audio>` source → `audio.play()`
    * handle autoplay lock: if play() rejects, set `audioLocked=true`
* Add UI controls:

  * Button: “Enable Audio” (calls a short silent/short beep test or attempts play to unlock)
  * Toggle: “Auto Speak” (default ON)
  * Optional voice override field

Acceptance:

* Manual: enable speech + click “Enable Audio” + “Test Speak” → audio plays

Stop; update progress.md.

---

## Iteration 43 — Auto-speak: speak new insights using `tts_response` (core requirement)

Goal:

* When new insight arrives, UI automatically speaks `tts_response`.

Deliverables:

* Hook after insight receipt (where you already set `latestInsight`)
* Use these guards:

  * speak only if new insight (track last spoken `clip_id` or another stable unique id)
  * speak only if autoSpeak enabled
  * speak only if severity >= MED by default (configurable)
  * UI cooldown (e.g., 2000ms)
* Speak text:

  * `insight.summary.tts_response`
* Cancel previous in-flight speech:

  * abort fetch with `AbortController`
  * stop current audio playback and revoke old blob URL

Acceptance:

* Trigger a new insight → the UI speaks the `tts_response` automatically

Stop; update progress.md.

